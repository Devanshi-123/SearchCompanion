{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNC8jSDkq/9seTNIu240Rde",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Devanshi-123/SearchCompanion/blob/gh-pages/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4W9LmBXxaQG",
        "colab_type": "text"
      },
      "source": [
        "Animal Detection (Convulational Neural Network)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTs-3XauxlfH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "1f88d2a3-4678-44f6-e426-504dad034773"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czJRq-8cx3Mt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q keras"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NfwkVSwx7-P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d24319f6-99a1-4a8c-c637-b460b8ef23d8"
      },
      "source": [
        "import keras"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgLn3-sZyCWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing all te req lib\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras import backend as K\n",
        "from keras.preprocessing import image\n",
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV4eWX0NyFZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dimensions of our images.\n",
        "img_width, img_height = 28,28\n",
        "\n",
        "#defining the data directories\n",
        "train_data_dir= '/content/drive/My Drive/dataset/training_set'\n",
        "validation_data_dir= '/content/drive/My Drive/dataset/validation_set'\n",
        "n_training_sample= 400\n",
        "n_validation_sample= 100\n",
        "epochs=20\n",
        "batch_size=10"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmwo_TZFybrw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if K.image_data_format() == 'channels_first':\n",
        "    input_shape = (3, img_width, img_height)\n",
        "else:\n",
        "    input_shape = (img_width, img_height, 3)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Fc-Q5boydZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defining the model\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(), \n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu), \n",
        "  tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(2, activation=tf.nn.sigmoid)])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcEmcMXZyiTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#defining the optimizer and metrics\n",
        "model.compile(optimizer = tf.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01Y_cLIFyoHB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "ae1d83e3-78c6-4e28-d438-d08b45d09c72"
      },
      "source": [
        "# this is the augmentation configuration we will use for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "# this is the augmentation configuration we will use for testing:\n",
        "# only rescaling\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=n_training_sample // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=n_validation_sample // batch_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8006 images belonging to 2 classes.\n",
            "Found 1002 images belonging to 2 classes.\n",
            "WARNING:tensorflow:From <ipython-input-9-b6a26b63a4b7>:29: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/20\n",
            "40/40 [==============================] - 302s 8s/step - loss: 0.6984 - accuracy: 0.4700 - val_loss: 0.6939 - val_accuracy: 0.4500\n",
            "Epoch 2/20\n",
            "40/40 [==============================] - 266s 7s/step - loss: 0.6932 - accuracy: 0.5475 - val_loss: 0.6933 - val_accuracy: 0.3800\n",
            "Epoch 3/20\n",
            "40/40 [==============================] - 263s 7s/step - loss: 0.6930 - accuracy: 0.5075 - val_loss: 0.6926 - val_accuracy: 0.5700\n",
            "Epoch 4/20\n",
            "40/40 [==============================] - 247s 6s/step - loss: 0.6944 - accuracy: 0.4850 - val_loss: 0.6931 - val_accuracy: 0.5200\n",
            "Epoch 5/20\n",
            "40/40 [==============================] - 225s 6s/step - loss: 0.6931 - accuracy: 0.5275 - val_loss: 0.6931 - val_accuracy: 0.5400\n",
            "Epoch 6/20\n",
            "40/40 [==============================] - 224s 6s/step - loss: 0.6931 - accuracy: 0.5200 - val_loss: 0.6931 - val_accuracy: 0.5600\n",
            "Epoch 7/20\n",
            "40/40 [==============================] - 195s 5s/step - loss: 0.6931 - accuracy: 0.5400 - val_loss: 0.6931 - val_accuracy: 0.5300\n",
            "Epoch 8/20\n",
            "40/40 [==============================] - 191s 5s/step - loss: 0.6931 - accuracy: 0.5375 - val_loss: 0.6931 - val_accuracy: 0.4100\n",
            "Epoch 9/20\n",
            "40/40 [==============================] - 187s 5s/step - loss: 0.6931 - accuracy: 0.4925 - val_loss: 0.6931 - val_accuracy: 0.5300\n",
            "Epoch 10/20\n",
            "40/40 [==============================] - 184s 5s/step - loss: 0.6931 - accuracy: 0.5450 - val_loss: 0.6931 - val_accuracy: 0.4200\n",
            "Epoch 11/20\n",
            "40/40 [==============================] - 179s 4s/step - loss: 0.6931 - accuracy: 0.4825 - val_loss: 0.6931 - val_accuracy: 0.4400\n",
            "Epoch 12/20\n",
            "40/40 [==============================] - 150s 4s/step - loss: 0.6931 - accuracy: 0.5325 - val_loss: 0.6931 - val_accuracy: 0.4700\n",
            "Epoch 13/20\n",
            "40/40 [==============================] - 139s 3s/step - loss: 0.6931 - accuracy: 0.5150 - val_loss: 0.6931 - val_accuracy: 0.5100\n",
            "Epoch 14/20\n",
            "40/40 [==============================] - 139s 3s/step - loss: 0.6932 - accuracy: 0.5100 - val_loss: 0.6931 - val_accuracy: 0.5800\n",
            "Epoch 15/20\n",
            "40/40 [==============================] - 116s 3s/step - loss: 0.6931 - accuracy: 0.5575 - val_loss: 0.6931 - val_accuracy: 0.5400\n",
            "Epoch 16/20\n",
            "40/40 [==============================] - 115s 3s/step - loss: 0.6931 - accuracy: 0.5625 - val_loss: 0.6931 - val_accuracy: 0.4800\n",
            "Epoch 17/20\n",
            "40/40 [==============================] - 109s 3s/step - loss: 0.6931 - accuracy: 0.5575 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
            "Epoch 18/20\n",
            "40/40 [==============================] - 109s 3s/step - loss: 0.6931 - accuracy: 0.5375 - val_loss: 0.6931 - val_accuracy: 0.4900\n",
            "Epoch 19/20\n",
            "40/40 [==============================] - 100s 3s/step - loss: 0.6932 - accuracy: 0.5175 - val_loss: 0.6931 - val_accuracy: 0.3900\n",
            "Epoch 20/20\n",
            "40/40 [==============================] - 109s 3s/step - loss: 0.6931 - accuracy: 0.5300 - val_loss: 0.6931 - val_accuracy: 0.5300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f158af519e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDtKtdtTywx_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8aa1f11-d276-4205-d6f4-671128d35bee"
      },
      "source": [
        "#testing the model\n",
        "pred= image.load_img('/content/drive/My Drive/dataset/test_set/cats/cat.4502.jpg', target_size=(28,28))\n",
        "pred=image.img_to_array(pred)\n",
        "pred= np.expand_dims(pred, axis=0)\n",
        "result= model.predict(pred)\n",
        "print(result)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bBsNAV3y86J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4edb4add-762b-4ba5-e7e0-da5c92e5f1ee"
      },
      "source": [
        "if result[0][0]==1:\n",
        "    answer='Cat'\n",
        "else:\n",
        "    answer='Dog'\n",
        "print(\"The animal in the image is\",answer)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The animal in the image is Cat\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}